{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e549fa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import csv\n",
    "import pandas as pd\n",
    "import glob\n",
    "import datetime\n",
    "import unidecode\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "\n",
    "# replace 'filename.csv' with the name of your CSV file\n",
    "df = pd.read_csv('/scratch/students/bousbina/corpus/USA/df_nyh.csv')\n",
    "df = df.sort_values('date', ascending=True)\n",
    "\n",
    "# display the first few rows of the DataFrame\n",
    "df.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Get list of all articles\n",
    "# every_article = glob.glob('/scratch/students/bousbina/corpus/USA/filtered_data.csv')\n",
    "# dates_us = set()\n",
    "\n",
    "# for article in tqdm(every_article):\n",
    "#     date = article.split(\"/\")[-1][:10]\n",
    "#     dates_us.add(date)\n",
    "    \n",
    "# with open('/scratch/students/bousbina/corpus/USA/dates_us.txt', 'w') as f:\n",
    "#     # parse dates as datetime objects and sort them\n",
    "#     dates_us_sorted = sorted([datetime.datetime.strptime(date, '%Y/%m/%d') for date in dates_us])\n",
    "#     # convert datetime objects back to strings in the desired format and write them to file\n",
    "#     f.write('\\n'.join([date.strftime('%Y/%m/%d') for date in dates_us_sorted]))\n",
    "\n",
    "\n",
    "print(\"Total number of articles : \" ,df.shape[0])\n",
    "\n",
    "filtered_df = df[df['text'].str.contains('futur')]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "filtered_df.to_csv('/scratch/students/bousbina/corpus/USA/filtered_data.csv', index=False)\n",
    "\n",
    "filtered_df.head()\n",
    "\n",
    "\n",
    "print(\"Total number of articles filtered  : \" ,filtered_df.shape[0])\n",
    "\n",
    "filtered_df.columns\n",
    "\n",
    "\n",
    "## Counting raw \"future\" and \"futur\" frequency in the corpus\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "filtered_df[\"date\"] = pd.to_datetime(filtered_df[\"date\"], format=\"%Y/%m/%d\").dt.date\n",
    "filtered_df = filtered_df.set_index(\"date\")\n",
    "x= np.arange(len(df))\n",
    "\n",
    "\n",
    "#We define a function that counts raw frequency \n",
    "def count_word_frequency(word, text):\n",
    "    return text.count(word)\n",
    "\n",
    "#Create a new column in the dataframe for the raw frequency of the word in each row using the apply()\n",
    "#method and the count_word_frequency() function.\n",
    "\n",
    "word = \"futur\"\n",
    "filtered_df[\"frequency\"] = filtered_df[\"text\"].apply(lambda x: count_word_frequency(word, x))\n",
    "\n",
    "\n",
    "\n",
    "# filtered_df[\"futur_frequency\"] = filtered_df['text'].str.count('future')\n",
    "\n",
    "# # Get the raw frequency by summing up the counts from all rows\n",
    "# raw_frequency = filtered_df['futur_frequency'].sum()\n",
    "\n",
    "# freq_by_date = filtered_df.groupby('date')['futur_frequency'].sum()\n",
    "\n",
    "# Create a scatter plot of the frequency counts vs. date\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(filtered_df.index, filtered_df[\"frequency\"], label=\"Raw frequencies\")\n",
    "# plt.scatter(freq_by_date.index, freq_by_date.values)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Frequency of the word \"future\"')\n",
    "plt.title(' Raw frequency of the word \"future\"')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Fit a linear regression model to the data\n",
    "model = LinearRegression()\n",
    "X = np.arange(len(filtered_df)).reshape(-1, 1)\n",
    "y = filtered_df['frequency'].values.reshape(-1, 1)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Add a regression line to the scatter plot\n",
    "plt.plot(filtered_df.index, model.predict(X), label=\"Linear regression\")\n",
    "plt.title(' Raw frequency of the word \"future\" according to Linear regression')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "## Observation of the trigram \"the future is\" in four different blocks:\n",
    "### [1840-1860] ,[1860-1880], [1880-1900], [1900-1920]\n",
    "\n",
    "import nltk \n",
    "import datetime\n",
    "nltk.download('punkt')\n",
    "from nltk import ngrams\n",
    "from nltk.text import Text\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from nltk.collocations import BigramAssocMeasures, TrigramAssocMeasures\n",
    "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
    "\n",
    "\n",
    "\n",
    "# Load the CSV file into a pandas data frame\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "df = pd.read_csv(\"/scratch/students/bousbina/corpus/USA/filtered_data.csv\")\n",
    "df['date'] = pd.to_datetime(df['date'], format='%Y/%m/%d').dt.date\n",
    "\n",
    "# Define the date ranges\n",
    "range1 = (datetime.datetime.strptime('1840/01/01', '%Y/%m/%d').date(),\n",
    "          datetime.datetime.strptime('1860/12/31', '%Y/%m/%d').date())\n",
    "\n",
    "range2 = (datetime.datetime.strptime('1860/01/01', '%Y/%m/%d').date(),\n",
    "          datetime.datetime.strptime('1880/12/31', '%Y/%m/%d').date())\n",
    "\n",
    "range3 = (datetime.datetime.strptime('1880/01/01', '%Y/%m/%d').date(),\n",
    "          datetime.datetime.strptime('1900/12/31', '%Y/%m/%d').date())\n",
    "\n",
    "range4 = (datetime.datetime.strptime('1900/01/01', '%Y/%m/%d').date(),\n",
    "          datetime.datetime.strptime('1920/12/31', '%Y/%m/%d').date())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "block1 = df.loc[df[\"date\"].between(range1[0], range1[1])]\n",
    "new_value1 = block1[\"text\"].apply(nltk.word_tokenize) \n",
    "block1.loc[:, \"tokens\"] =  new_value1\n",
    "\n",
    "# Print the number of dates in each block\n",
    "print(f\"Block 1 ({range1[0]} - {range1[1]}): {len(block1)} dates\")\n",
    "\n",
    "\n",
    "block2 = df.loc[df[\"date\"].between(range2[0], range2[1])]\n",
    "new_value2 = block2[\"text\"].apply(nltk.word_tokenize) \n",
    "block2.loc[:, \"tokens\"] =  new_value2\n",
    "\n",
    "\n",
    "print(f\"Block 2 ({range2[0]} - {range2[1]}): {len(block2)} dates\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "block3 = df.loc[df[\"date\"].between(range3[0], range3[1])]\n",
    "new_value3 = block3[\"text\"].apply(nltk.word_tokenize) \n",
    "block3.loc[:, \"tokens\"] =  new_value3\n",
    "\n",
    "\n",
    "print(f\"Block 3 ({range3[0]} - {range3[1]}): {len(block3)} dates\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "block4 = df.loc[df[\"date\"].between(range4[0], range4[1])]\n",
    "new_value4 = block4[\"text\"].apply(nltk.word_tokenize) \n",
    "block4.loc[:, \"tokens\"] =  new_value4\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Block 4 ({range4[0]} - {range4[1]}): {len(block4)} dates\")\n",
    "\n",
    "import pandas as pd\n",
    "from nltk import FreqDist\n",
    "\n",
    "\n",
    "\n",
    "# Create a list of tri-grams in the tokenized text for each block\n",
    "tri_grams_block_1 = [tri for doc in block1[\"tokens\"] for tri in nltk.ngrams(doc, 3) if ((\"the\",\"future\", \"is\") == tri) or ((\"the\",\"futur\", \"is\") == tri)]\n",
    "tri_grams_block_2 = [tri for doc in block2[\"tokens\"] for tri in nltk.ngrams(doc, 3) if ((\"the\",\"future\", \"is\") == tri) or ((\"the\",\"futur\", \"is\") == tri)]\n",
    "tri_grams_block_3 = [tri for doc in block3[\"tokens\"] for tri in nltk.ngrams(doc, 3) if ((\"the\",\"future\", \"is\") == tri) or ((\"the\",\"futur\", \"is\") == tri)]\n",
    "tri_grams_block_4 = [tri for doc in block4[\"tokens\"] for tri in nltk.ngrams(doc, 3) if ((\"the\",\"future\", \"is\") == tri) or ((\"the\",\"futur\", \"is\") == tri)]\n",
    "\n",
    "# Compute the frequency distribution of the tri-grams for each block\n",
    "freq_dist_block_1 = FreqDist(tri_grams_block_1)\n",
    "freq_dist_block_2 = FreqDist(tri_grams_block_2)\n",
    "freq_dist_block_3 = FreqDist(tri_grams_block_3)\n",
    "freq_dist_block_4 = FreqDist(tri_grams_block_4)\n",
    "\n",
    "#Print all occurrences of the tri-gram \"le future est\" in each block of tokenized text,\n",
    "print(\"Block 1 (1840-1860):\")\n",
    "print(tri_grams_block_1)\n",
    "print(\"Block 2 (1860-1880):\")\n",
    "print(tri_grams_block_2)\n",
    "print(\"Block 3 (1880-1900):\")\n",
    "print(tri_grams_block_3)\n",
    "print(\"Block 4 (1900-1920):\")\n",
    "print(tri_grams_block_4)\n",
    "\n",
    "\n",
    "#find the adjectifve after le future est\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Most common occurences in the blocks\n",
    "\n",
    "# Print the five most common word uses of the tri-gram \"le future est\" in each block\n",
    "print(\"Block 1 (1840-1860):\")\n",
    "print(freq_dist_block_1.most_common(5))\n",
    "print(\"Block 2 (1860-1880):\")\n",
    "print(freq_dist_block_2.most_common(5))\n",
    "print(\"Block 3 (1880-1900):\")\n",
    "print(freq_dist_block_3.most_common(5))\n",
    "print(\"Block 4 (1900-1920):\")\n",
    "print(freq_dist_block_4.most_common(5))\n",
    "\n",
    "### Occurences of  the two grams before and after “ futur  ”\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "import string \n",
    "\n",
    "import pandas as pd\n",
    "from nltk import FreqDist\n",
    "\n",
    "# Create a list of two-grams in the tokenized text for block 1\n",
    "two_grams_block_1_before = []\n",
    "two_grams_block_1_after = []\n",
    "\n",
    "punctuations = string.punctuation\n",
    "\n",
    "\n",
    "\n",
    "for doc in block1[\"tokens\"]:\n",
    "    doc = [word for word in doc if word not in punctuations and not word.isdigit()]\n",
    "    for i in range(len(doc)):\n",
    "        if doc[i] == (\"futur\"):\n",
    "            if i > 1:\n",
    "                two_grams_block_1_before.append((doc[i-2], doc[i-1]))\n",
    "            if i < len(doc)-3:\n",
    "                two_grams_block_1_after.append((doc[i+1], doc[i+2]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Compute the frequency distribution of the tri-grams for block 1\n",
    "freq_dist_block_1_before = FreqDist(two_grams_block_1_before)\n",
    "freq_dist_block_1_after = FreqDist(two_grams_block_1_after)\n",
    "\n",
    "\n",
    "with open(\"two-grams-output_us.txt\", \"w\") as file:\n",
    "    # Write the two-grams before \"futur\"\n",
    "    file.write(\"-------------------------BLOCK_1-----------------------Two-grams before 'futur':\\n\")\n",
    "    for two_gram in two_grams_block_1_before:\n",
    "        file.write(f\"{two_gram[0]} {two_gram[1]}\\n\")\n",
    "    # Write the two-grams after \"futur\"\n",
    "    file.write(\"-------------------------BLOCK_1-----------------------Two-grams after 'futur':\\n\")\n",
    "    for two_gram in two_grams_block_1_after:\n",
    "        file.write(f\"{two_gram[0]} {two_gram[1]}\\n\")\n",
    "\n",
    "# Print all occurrences of the tri-gram \"l'avenir\" in block of tokenized text\n",
    "print(\"Block 1 (1840-1860): \\n\")\n",
    "print(f\"2-grams before 'le futur': \\n\")\n",
    "print(two_grams_block_1_before, \"\\n\")\n",
    "print(f\"2-grams after 'le futur': \\n\")\n",
    "print(two_grams_block_1_after, \"\\n\")\n",
    "print(\"Most common 2-grams before: \\n\")\n",
    "print(freq_dist_block_1_before.most_common(), \"\\n\")\n",
    "print(\"Most common 2-grams after: \\n\")\n",
    "print(freq_dist_block_1_after.most_common())\n",
    "\n",
    "# Create a list of two-grams in the tokenized text for block 2\n",
    "two_grams_block_2_before = []\n",
    "two_grams_block_2_after = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for doc in block2[\"tokens\"]:\n",
    "    doc = [word for word in doc if word not in punctuations and not word.isdigit()]\n",
    "    for i in range(len(doc)):\n",
    "        if doc[i] == (\"futur\"):\n",
    "            if i > 1:\n",
    "                two_grams_block_2_before.append((doc[i-2], doc[i-1]))\n",
    "            if i < len(doc)-3:\n",
    "                two_grams_block_2_after.append((doc[i+1], doc[i+2]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Compute the frequency distribution of the tri-grams for block 1\n",
    "freq_dist_block_2_before = FreqDist(two_grams_block_2_before)\n",
    "freq_dist_block_2_after = FreqDist(two_grams_block_2_after)\n",
    "\n",
    "\n",
    "\n",
    "with open(\"two-grams-output_us.txt\", \"a\") as file:\n",
    "    # Write the two-grams before \"futur\"\n",
    "    file.write(\"-------------------------BLOCK_2-----------------------Two-grams before 'futur':\\n\")\n",
    "    for two_gram in two_grams_block_2_before:\n",
    "        file.write(f\"{two_gram[0]} {two_gram[1]}\\n\")\n",
    "    # Write the two-grams after \"futur\"\n",
    "    file.write(\"-------------------------BLOCK_2-----------------------Two-grams after 'futur':\\n\")\n",
    "    for two_gram in two_grams_block_2_after:\n",
    "        file.write(f\"{two_gram[0]} {two_gram[1]}\\n\")\n",
    "\n",
    "\n",
    "# Print all occurrences of the tri-gram \"l'avenir\" in block of tokenized text\n",
    "print(\"Block 2 (1860-1880): \\n\")\n",
    "print(f\"2-grams before 'le futur': \\n\")\n",
    "print(two_grams_block_2_before, \"\\n\")\n",
    "print(f\"2-grams after 'le futur': \\n\")\n",
    "print(two_grams_block_2_after, \"\\n\")\n",
    "print(\"Most common 2-grams before: \\n\")\n",
    "print(freq_dist_block_2_before.most_common(), \"\\n\")\n",
    "print(\"Most common 2-grams after: \\n\")\n",
    "print(freq_dist_block_2_after.most_common())\n",
    "\n",
    "# Create a list of two-grams in the tokenized text for block 2\n",
    "two_grams_block_3_before = []\n",
    "two_grams_block_3_after = []\n",
    "\n",
    "\n",
    "\n",
    "for doc in block3[\"tokens\"]:\n",
    "    doc = [word for word in doc if word not in punctuations and not word.isdigit()]\n",
    "    for i in range(len(doc)):\n",
    "        if doc[i] == (\"futur\"):\n",
    "            if i > 1:\n",
    "                two_grams_block_3_before.append((doc[i-2], doc[i-1]))\n",
    "            if i < len(doc)-3:\n",
    "                two_grams_block_3_after.append((doc[i+1], doc[i+2]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Compute the frequency distribution of the tri-grams for block 1\n",
    "freq_dist_block_3_before = FreqDist(two_grams_block_3_before)\n",
    "freq_dist_block_3_after = FreqDist(two_grams_block_3_after)\n",
    "\n",
    "with open(\"two-grams-output.txt_us\", \"a\") as file:\n",
    "    # Write the two-grams before \"futur\"\n",
    "    file.write(\"-------------------------BLOCK_3-----------------------Two-grams before 'futur':\\n\")\n",
    "    for two_gram in two_grams_block_2_before:\n",
    "        file.write(f\"{two_gram[0]} {two_gram[1]}\\n\")\n",
    "    # Write the two-grams after \"futur\"\n",
    "    file.write(\"-------------------------BLOCK_3-----------------------Two-grams after 'futur':\\n\")\n",
    "    for two_gram in two_grams_block_2_after:\n",
    "        file.write(f\"{two_gram[0]} {two_gram[1]}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# Print all occurrences of the tri-gram \"l'avenir\" in block of tokenized text\n",
    "print(\"Block 3 (1880-1900): \\n\")\n",
    "print(f\"2-grams before 'le futur': \\n\")\n",
    "print(two_grams_block_3_before, \"\\n\")\n",
    "print(f\"2-grams after 'le futur': \\n\")\n",
    "print(two_grams_block_3_after, \"\\n\")\n",
    "print(\"Most common 2-grams before: \\n\")\n",
    "print(freq_dist_block_3_before.most_common(), \"\\n\")\n",
    "print(\"Most common 2-grams after: \\n\")\n",
    "print(freq_dist_block_3_after.most_common())\n",
    "\n",
    "# Create a list of two-grams in the tokenized text for block 2\n",
    "two_grams_block_4_before = []\n",
    "two_grams_block_4_after = []\n",
    "\n",
    "\n",
    "\n",
    "for doc in block4[\"tokens\"]:\n",
    "    doc = [word for word in doc if word not in punctuations and not word.isdigit()]\n",
    "    for i in range(len(doc)):\n",
    "        if doc[i] == (\"futur\"):\n",
    "            if i > 1:\n",
    "                two_grams_block_4_before.append((doc[i-2], doc[i-1]))\n",
    "            if i < len(doc)-3:\n",
    "                two_grams_block_4_after.append((doc[i+1], doc[i+2]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Compute the frequency distribution of the tri-grams for block 1\n",
    "freq_dist_block_4_before = FreqDist(two_grams_block_4_before)\n",
    "freq_dist_block_4_after = FreqDist(two_grams_block_4_after)\n",
    "\n",
    "with open(\"two-grams-output_us.txt\", \"a\") as file:\n",
    "    # Write the two-grams before \"futur\"\n",
    "    file.write(\"-------------------------BLOCK_4-----------------------Two-grams before 'futur':\\n\")\n",
    "    for two_gram in two_grams_block_2_before:\n",
    "        file.write(f\"{two_gram[0]} {two_gram[1]}\\n\")\n",
    "    # Write the two-grams after \"futur\"\n",
    "    file.write(\"-------------------------BLOCK_4-----------------------Two-grams after 'futur':\\n\")\n",
    "    for two_gram in two_grams_block_2_after:\n",
    "        file.write(f\"{two_gram[0]} {two_gram[1]}\\n\")\n",
    "\n",
    "\n",
    "# Print all occurrences of the tri-gram \"l'avenir\" in block of tokenized text\n",
    "print(\"Block 4 (1900-1920): \\n\")\n",
    "print(f\"2-grams before 'le futur': \\n\")\n",
    "print(two_grams_block_4_before, \"\\n\")\n",
    "print(f\"2-grams after 'le futur': \\n\")\n",
    "print(two_grams_block_4_after, \"\\n\")\n",
    "print(\"Most common 2-grams before: \\n\")\n",
    "print(freq_dist_block_4_before.most_common(), \"\\n\")\n",
    "print(\"Most common 2-grams after: \\n\")\n",
    "print(freq_dist_block_4_after.most_common())\n",
    "\n",
    "## Concordance analysis: \n",
    "\n",
    "### A concordance analysis can help us identify the contexts in which the word 'futur' appears. We use the NLTK library that has a Text.concordance() method that we can use to generate a concordance for a word in our corpus.\n",
    "\n",
    "from nltk.text import Text\n",
    "\n",
    "# Create a list of token strings for each block\n",
    "block1_tokens = [token for doc in block1[\"tokens\"] for token in doc]\n",
    "block2_tokens = [token for doc in block2[\"tokens\"] for token in doc]\n",
    "block3_tokens = [token for doc in block3[\"tokens\"] for token in doc]\n",
    "block4_tokens = [token for doc in block4[\"tokens\"] for token in doc]\n",
    "\n",
    "# Create a Text object for each block\n",
    "text_block_1 = Text(block1_tokens)\n",
    "text_block_2 = Text(block2_tokens)\n",
    "text_block_3 = Text(block3_tokens)\n",
    "text_block_4 = Text(block4_tokens)\n",
    "\n",
    "\n",
    "with open(\"key-word-in-context-futur_us.txt \", \"w\") as file :\n",
    "    file.write(\"----------------------BLOCK-1-------------------------- \\n\")\n",
    "    for line in text_block_1.concordance_list(\"futur\", lines=22):\n",
    "        file.write(line.line + \"\\n\")\n",
    "    file.write(\"----------------------BLOCK-2-------------------------- \\n\")\n",
    "    for line in text_block_2.concordance_list(\"futur\", lines=727):\n",
    "        file.write(line.line + \"\\n\")\n",
    "    file.write(\"----------------------BLOCK-3-------------------------- \\n\")\n",
    "    for line in text_block_3.concordance_list(\"futur\", lines=793):\n",
    "        file.write(line.line + \"\\n\")\n",
    "    file.write(\"----------------------BLOCK-4-------------------------- \\n\")\n",
    "    for line in text_block_4.concordance_list(\"futur\", lines=708):\n",
    "        file.write(line.line + \"\\n\")\n",
    "    \n",
    "\n",
    "# Perform a concordance analysis for the word \"futur\" in each block\n",
    "print(\"Concordance analysis for 'futur' in Block 1:\")\n",
    "text_block_1.concordance(\"futur\")\n",
    "print(\"\\nConcordance analysis for 'futur' in Block 2:\")\n",
    "text_block_2.concordance(\"futur\")\n",
    "print(\"\\nConcordance analysis for 'futur' in Block 3:\")\n",
    "text_block_3.concordance(\"futur\")\n",
    "print(\"\\nConcordance analysis for 'futur' in Block 4:\")\n",
    "text_block_4.concordance(\"futur\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
